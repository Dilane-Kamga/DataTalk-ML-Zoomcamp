





# Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

# Load the csv file
df = pd.read_csv('bank/bank-full.csv', delimiter=';')

# Display the first few rows of the dataset
df.head()





# Select only the required columns
columns = ['age', 'job', 'marital', 'education', 'balance', 'housing', 'contact',
           'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']
df = df[columns]

# Check for missing values
df.isnull().sum()





# Find the mode of the education column
education_mode = df['education'].mode()[0]
print(f"The mode for the education column is: {education_mode}")





df.info()


# Select numerical features
numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Compute the correlation matrix
corr_matrix = df[numerical_features].corr()

# Plot the correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features")
plt.show()





# Encode target variable y (yes/no to 1/0)
df['y'] = df['y'].map({'yes': 1, 'no': 0})


df.head()





# Extract the target column
y = df['y']

# Drop the target column from the DataFrame to get the features DataFrame
df_features = df.drop(columns=['y'])


# First, split the data into 80% train and 20% test
X_train_full, X_test, y_train_full, y_test = train_test_split(df_features, y, test_size=0.2, random_state=42)

# Now split the 80% train data into 75% train and 25% validation (which is 60% and 20% of the original data respectively)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)

print(f"Train set: {len(X_train)} samples")
print(f"Validation set: {len(X_val)} samples")
print(f"Test set: {len(X_test)} samples")





from sklearn.metrics import mutual_info_score

# Extract categorical features
categorical_features = ['job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome']

# Create the function mutual information
def mutual_info_y_score(series):
    return mutual_info_score(series, y_train)


# Mutual information score calculation
mi_scores = X_train[categorical_features].apply(mutual_info_y_score)
mi_scores = round(mi_scores, 2)
mi_scores.sort_values(ascending=False)





from sklearn.feature_extraction import DictVectorizer

# Initialize vectorizer
one_hot_encoder = DictVectorizer(sparse=False)

# Convert training set to a dictionary
train_dict = X_train.to_dict(orient='records')

# Train the vectorizer and transform features
X_train = one_hot_encoder.fit_transform(train_dict)

# Check the features
one_hot_encoder.get_feature_names_out()


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Train logistic regression model
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Transform features for validation set
val_dict = X_val.to_dict(orient='records')
X_val = one_hot_encoder.transform(val_dict)

# Validate the model
y_pred = model.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation accuracy: {accuracy:.2f}")





# Train model with all features (baseline)
baseline_accuracy = accuracy_score(y_val, model.predict(X_val_full))

# Evaluate feature elimination
features = X_train_full.columns
accuracy_diffs = {}

for feature in features:
    X_train_new = X_train_full.drop(columns=[feature])
    X_val_new = X_val_full.drop(columns=[feature])
    
    model.fit(X_train_new, y_train)
    accuracy_new = accuracy_score(y_val, model.predict(X_val_new))
    accuracy_diffs[feature] = baseline_accuracy - accuracy_new

# Find feature with the smallest difference
smallest_diff_feature = min(accuracy_diffs, key=accuracy_diffs.get)
print(f"Feature with the smallest difference: {smallest_diff_feature}")





# Try different values of C for regularization
C_values = [0.01, 0.1, 1, 10, 100]
best_C = None
best_accuracy = 0

for C in C_values:
    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)
    model.fit(X_train_full, y_train)
    accuracy = accuracy_score(y_val, model.predict(X_val_full))
    print(f"C={C}: Validation accuracy = {accuracy:.3f}")
    
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_C = C

print(f"Best C value: {best_C}")






