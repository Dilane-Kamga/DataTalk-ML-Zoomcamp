





# Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

# Load the csv file
df = pd.read_csv('bank/bank-full.csv', delimiter=';')

# Display the first few rows of the dataset
df.head()





# Select only the required columns
columns = ['age', 'job', 'marital', 'education', 'balance', 'housing', 'contact',
           'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']
df = df[columns]

# Check for missing values
df.isnull().sum()





# Find the mode of the education column
education_mode = df['education'].mode()[0]
print(f"The mode for the education column is: {education_mode}")





# Select numerical features
numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

# Compute the correlation matrix
corr_matrix = df[numerical_features].corr()

# Plot the correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features")
plt.show()





# Encode target variable y (yes/no to 1/0)
df['y'] = df['y'].map({'yes': 1, 'no': 0})





# Split the data into training, validation, and test sets (60%/20%/20%)
df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)
df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)





from sklearn.feature_selection import mutual_info_classif

# Extract categorical features
categorical_features = ['job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome']

# Mutual information score calculation
X_train = df_train[categorical_features]
y_train = df_train['y']

# Convert categorical features to numerical using one-hot encoding
X_train_encoded = pd.get_dummies(X_train, drop_first=True)

# Calculate mutual information score
mi_scores = mutual_info_classif(X_train_encoded, y_train)
mi_scores = pd.Series(mi_scores, index=X_train_encoded.columns).sort_values(ascending=False)

# Display the mutual information scores
mi_scores.round(2)





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# One-hot encode categorical features for logistic regression
X_train_full = pd.get_dummies(df_train.drop('y', axis=1), drop_first=True)
X_val_full = pd.get_dummies(df_val.drop('y', axis=1), drop_first=True)

y_train = df_train['y']
y_val = df_val['y']

# Train logistic regression model
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
model.fit(X_train_full, y_train)

# Validate the model
y_pred = model.predict(X_val_full)
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation accuracy: {accuracy:.2f}")





# Train model with all features (baseline)
baseline_accuracy = accuracy_score(y_val, model.predict(X_val_full))

# Evaluate feature elimination
features = X_train_full.columns
accuracy_diffs = {}

for feature in features:
    X_train_new = X_train_full.drop(columns=[feature])
    X_val_new = X_val_full.drop(columns=[feature])
    
    model.fit(X_train_new, y_train)
    accuracy_new = accuracy_score(y_val, model.predict(X_val_new))
    accuracy_diffs[feature] = baseline_accuracy - accuracy_new

# Find feature with the smallest difference
smallest_diff_feature = min(accuracy_diffs, key=accuracy_diffs.get)
print(f"Feature with the smallest difference: {smallest_diff_feature}")





# Try different values of C for regularization
C_values = [0.01, 0.1, 1, 10, 100]
best_C = None
best_accuracy = 0

for C in C_values:
    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)
    model.fit(X_train_full, y_train)
    accuracy = accuracy_score(y_val, model.predict(X_val_full))
    print(f"C={C}: Validation accuracy = {accuracy:.3f}")
    
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_C = C

print(f"Best C value: {best_C}")






